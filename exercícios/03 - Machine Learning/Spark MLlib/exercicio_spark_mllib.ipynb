{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d12bae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+-----+----+\n",
      "|survived|status|  age| sex|\n",
      "+--------+------+-----+----+\n",
      "|     yes| first|adult|male|\n",
      "|     yes| first|adult|male|\n",
      "|     yes| first|adult|male|\n",
      "|     yes| first|adult|male|\n",
      "|     yes| first|adult|male|\n",
      "|     yes| first|adult|male|\n",
      "|     yes| first|adult|male|\n",
      "|     yes| first|adult|male|\n",
      "|     yes| first|adult|male|\n",
      "|     yes| first|adult|male|\n",
      "|     yes| first|adult|male|\n",
      "|     yes| first|adult|male|\n",
      "|     yes| first|adult|male|\n",
      "|     yes| first|adult|male|\n",
      "|     yes| first|adult|male|\n",
      "|     yes| first|adult|male|\n",
      "|     yes| first|adult|male|\n",
      "|     yes| first|adult|male|\n",
      "|     yes| first|adult|male|\n",
      "|     yes| first|adult|male|\n",
      "+--------+------+-----+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder.remote(\"sc://192.168.0.13:15002\")\n",
    "    .appName(\"hello_spark\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "df = spark.read.csv(\n",
    "    \"hdfs://192.168.0.13:9000/titanic.csv\",\n",
    "    header=True,\n",
    "    inferSchema=True,\n",
    ")\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6855a62",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnsupportedOperationException",
     "evalue": "17: {\n  1: {\n    1: {\n      1: \"org.apache.spark.ml.feature.OneHotEncoder\"\n      2: \"OneHotEncoder_c784fa638978\"\n      3: 1\n    }\n    2: {\n      1: {\n        1: {\n          13: 0x736c6f437475706e\n        }\n        2: {\n          22: {\n            1: {\n              11: {\n                2: \"UTF8_BINARY\"\n              }\n            }\n            2: {\n              13: \"status\"\n            }\n            2: {\n              13: \"age\"\n            }\n            2: {\n              13: \"sex\"\n            }\n          }\n        }\n      }\n      1: {\n        1: \"outputCols\"\n        2: {\n          22: {\n            1: {\n              11: {\n                2: \"UTF8_BINARY\"\n              }\n            }\n            2: {\n              13: \"status_encoded\"\n            }\n            2: {\n              13: \"age_encoded\"\n            }\n            2: {\n              13: \"sex_encoded\"\n            }\n          }\n        }\n      }\n    }\n    3: {\n      1: {\n        2: 4\n      }\n      2: {\n        2: {\n          1: \"csv\"\n          2: {\n          }\n          3: {\n            1: \"header\"\n            2: \"true\"\n          }\n          3: {\n            1: \"inferSchema\"\n            2: \"true\"\n          }\n          4: \"hdfs://192.168.0.13:9000/titanic.csv\"\n        }\n      }\n    }\n  }\n}\n not supported.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnsupportedOperationException\u001b[0m             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mml\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OneHotEncoder\n\u001b[0;32m      3\u001b[0m encoder \u001b[38;5;241m=\u001b[39m OneHotEncoder(inputCols\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mage\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msex\u001b[39m\u001b[38;5;124m\"\u001b[39m], outputCols\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus_encoded\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mage_encoded\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msex_encoded\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m----> 5\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mencoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m encoded \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mtransform(df)\n\u001b[0;32m      7\u001b[0m encoded\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pyspark\\ml\\base.py:203\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[1;34m(self, dataset, params)\u001b[0m\n\u001b[0;32m    201\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[0;32m    202\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 203\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    205\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m    206\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    207\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params)\n\u001b[0;32m    208\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pyspark\\ml\\util.py:156\u001b[0m, in \u001b[0;36mtry_remote_fit.<locals>.wrapped\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    148\u001b[0m command \u001b[38;5;241m=\u001b[39m pb2\u001b[38;5;241m.\u001b[39mCommand()\n\u001b[0;32m    149\u001b[0m command\u001b[38;5;241m.\u001b[39mml_command\u001b[38;5;241m.\u001b[39mfit\u001b[38;5;241m.\u001b[39mCopyFrom(\n\u001b[0;32m    150\u001b[0m     pb2\u001b[38;5;241m.\u001b[39mMlCommand\u001b[38;5;241m.\u001b[39mFit(\n\u001b[0;32m    151\u001b[0m         estimator\u001b[38;5;241m=\u001b[39mestimator,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    154\u001b[0m     )\n\u001b[0;32m    155\u001b[0m )\n\u001b[1;32m--> 156\u001b[0m (_, properties, _) \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    157\u001b[0m model_info \u001b[38;5;241m=\u001b[39m deserialize(properties)\n\u001b[0;32m    158\u001b[0m client\u001b[38;5;241m.\u001b[39madd_ml_cache(model_info\u001b[38;5;241m.\u001b[39mobj_ref\u001b[38;5;241m.\u001b[39mid)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pyspark\\sql\\connect\\client\\core.py:1148\u001b[0m, in \u001b[0;36mSparkConnectClient.execute_command\u001b[1;34m(self, command, observations)\u001b[0m\n\u001b[0;32m   1146\u001b[0m     req\u001b[38;5;241m.\u001b[39muser_context\u001b[38;5;241m.\u001b[39muser_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_user_id\n\u001b[0;32m   1147\u001b[0m req\u001b[38;5;241m.\u001b[39mplan\u001b[38;5;241m.\u001b[39mcommand\u001b[38;5;241m.\u001b[39mCopyFrom(command)\n\u001b[1;32m-> 1148\u001b[0m data, _, metrics, observed_metrics, properties \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_and_fetch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1149\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobservations\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\n\u001b[0;32m   1150\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1151\u001b[0m \u001b[38;5;66;03m# Create a query execution object.\u001b[39;00m\n\u001b[0;32m   1152\u001b[0m ei \u001b[38;5;241m=\u001b[39m ExecutionInfo(metrics, observed_metrics)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pyspark\\sql\\connect\\client\\core.py:1560\u001b[0m, in \u001b[0;36mSparkConnectClient._execute_and_fetch\u001b[1;34m(self, req, observations, self_destruct)\u001b[0m\n\u001b[0;32m   1557\u001b[0m properties: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Progress(handlers\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_progress_handlers, operation_id\u001b[38;5;241m=\u001b[39mreq\u001b[38;5;241m.\u001b[39moperation_id) \u001b[38;5;28;01mas\u001b[39;00m progress:\n\u001b[1;32m-> 1560\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m response \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_execute_and_fetch_as_iterator(\n\u001b[0;32m   1561\u001b[0m         req, observations, progress\u001b[38;5;241m=\u001b[39mprogress\n\u001b[0;32m   1562\u001b[0m     ):\n\u001b[0;32m   1563\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, StructType):\n\u001b[0;32m   1564\u001b[0m             schema \u001b[38;5;241m=\u001b[39m response\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pyspark\\sql\\connect\\client\\core.py:1537\u001b[0m, in \u001b[0;36mSparkConnectClient._execute_and_fetch_as_iterator\u001b[1;34m(self, req, observations, progress)\u001b[0m\n\u001b[0;32m   1535\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m kb\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m error:\n\u001b[1;32m-> 1537\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43merror\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pyspark\\sql\\connect\\client\\core.py:1811\u001b[0m, in \u001b[0;36mSparkConnectClient._handle_error\u001b[1;34m(self, error)\u001b[0m\n\u001b[0;32m   1809\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mthread_local\u001b[38;5;241m.\u001b[39minside_error_handling \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   1810\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(error, grpc\u001b[38;5;241m.\u001b[39mRpcError):\n\u001b[1;32m-> 1811\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle_rpc_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43merror\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error\n\u001b[0;32m   1813\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pyspark\\sql\\connect\\client\\core.py:1882\u001b[0m, in \u001b[0;36mSparkConnectClient._handle_rpc_error\u001b[1;34m(self, rpc_error)\u001b[0m\n\u001b[0;32m   1879\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m info\u001b[38;5;241m.\u001b[39mmetadata[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merrorClass\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mINVALID_HANDLE.SESSION_CHANGED\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   1880\u001b[0m                 \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_closed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m-> 1882\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m convert_exception(\n\u001b[0;32m   1883\u001b[0m                 info,\n\u001b[0;32m   1884\u001b[0m                 status\u001b[38;5;241m.\u001b[39mmessage,\n\u001b[0;32m   1885\u001b[0m                 \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fetch_enriched_error(info),\n\u001b[0;32m   1886\u001b[0m                 \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_display_server_stack_trace(),\n\u001b[0;32m   1887\u001b[0m             ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1889\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m SparkConnectGrpcException(status\u001b[38;5;241m.\u001b[39mmessage) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mUnsupportedOperationException\u001b[0m: 17: {\n  1: {\n    1: {\n      1: \"org.apache.spark.ml.feature.OneHotEncoder\"\n      2: \"OneHotEncoder_c784fa638978\"\n      3: 1\n    }\n    2: {\n      1: {\n        1: {\n          13: 0x736c6f437475706e\n        }\n        2: {\n          22: {\n            1: {\n              11: {\n                2: \"UTF8_BINARY\"\n              }\n            }\n            2: {\n              13: \"status\"\n            }\n            2: {\n              13: \"age\"\n            }\n            2: {\n              13: \"sex\"\n            }\n          }\n        }\n      }\n      1: {\n        1: \"outputCols\"\n        2: {\n          22: {\n            1: {\n              11: {\n                2: \"UTF8_BINARY\"\n              }\n            }\n            2: {\n              13: \"status_encoded\"\n            }\n            2: {\n              13: \"age_encoded\"\n            }\n            2: {\n              13: \"sex_encoded\"\n            }\n          }\n        }\n      }\n    }\n    3: {\n      1: {\n        2: 4\n      }\n      2: {\n        2: {\n          1: \"csv\"\n          2: {\n          }\n          3: {\n            1: \"header\"\n            2: \"true\"\n          }\n          3: {\n            1: \"inferSchema\"\n            2: \"true\"\n          }\n          4: \"hdfs://192.168.0.13:9000/titanic.csv\"\n        }\n      }\n    }\n  }\n}\n not supported."
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder\n",
    "\n",
    "encoder = OneHotEncoder(inputCol=\"status\", outputCol=\"status_encoded\")\n",
    "\n",
    "model = encoder.fit(df)\n",
    "encoded = model.transform(df)\n",
    "encoded.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
